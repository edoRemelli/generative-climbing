{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"]=\"PCI_BUS_ID\"\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"1\"\n",
    "\n",
    "import numpy as np\n",
    "import torch as T\n",
    "import torch.nn.functional as F\n",
    "from models_resnet import *\n",
    "import random\n",
    "from helper_functions import *\n",
    "import torchvision.utils as utils\n",
    "import argparse\n",
    "\n",
    "\n",
    "save_path = \"data/saved_models/saved_model.tar\"\n",
    "\n",
    "if not os.path.exists(\"data/saved_models\"):\n",
    "    os.makedirs(\"data/saved_models\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "parser = argparse.ArgumentParser()\n",
    "parser.add_argument('--batch_size', type=int, default=64)\n",
    "parser.add_argument('--epochs', type=int, default=301)\n",
    "parser.add_argument('--lr_e', type=float, default=0.0002)\n",
    "parser.add_argument('--lr_g', type=float, default=0.0002)\n",
    "parser.add_argument('--lr_d', type=float, default=0.0002)\n",
    "parser.add_argument(\"--num_workers\", type=int, default=4)\n",
    "parser.add_argument(\"--n_samples\", type=int, default=36)\n",
    "parser.add_argument('--n_z', type=int, default=20)\n",
    "parser.add_argument('--img_size', type=int, default=28)\n",
    "parser.add_argument('--w_kld', type=float, default=1)\n",
    "parser.add_argument('--w_loss_g', type=float, default=0.01)\n",
    "parser.add_argument('--w_loss_gd', type=float, default=1)\n",
    "\n",
    "def str2bool(v):\n",
    "    if v.lower() == 'true':\n",
    "        return True\n",
    "    else:\n",
    "        return False\n",
    "\n",
    "parser.add_argument('--resume_training', type=str2bool, default=False)\n",
    "parser.add_argument('--to_train', type=str2bool, default=True)\n",
    "parser.add_argument('--conditional', type=str2bool, default=False)\n",
    "\n",
    "opt = parser.parse_args([])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "manual_seed = random.randint(1, 10000)\n",
    "random.seed(manual_seed)\n",
    "T.manual_seed(manual_seed)\n",
    "if T.cuda.is_available():\n",
    "    T.cuda.manual_seed_all(manual_seed)\n",
    "\n",
    "train_loader = get_data_loader(opt, True)\n",
    "test_loader = get_data_loader(opt, False)\n",
    "\n",
    "E = get_cuda(Encoder(opt))\n",
    "G = get_cuda(Generator(opt)).apply(weights_init)\n",
    "D = get_cuda(Discriminator(opt)).apply(weights_init)\n",
    "\n",
    "device_ids = range(T.cuda.device_count())\n",
    "E = nn.DataParallel(E, device_ids)\n",
    "G = nn.DataParallel(G, device_ids)\n",
    "D = nn.DataParallel(D, device_ids)\n",
    "\n",
    "E_trainer = T.optim.Adam(E.parameters(), lr=opt.lr_e)\n",
    "G_trainer = T.optim.Adam(G.parameters(), lr=opt.lr_g, betas=(0.5, 0.999))\n",
    "D_trainer = T.optim.Adam(D.parameters(), lr=opt.lr_d, betas=(0.5, 0.999))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_batch(x_original, y_original):\n",
    "    batch_size = x_original.size(0)\n",
    "    y_real = get_cuda(T.ones(batch_size))\n",
    "    y_fake = get_cuda(T.zeros(batch_size))\n",
    "\n",
    "    #Extract latent_z corresponding to real images\n",
    "    z, mean, log_var = E(x_original, y_original)\n",
    "    kld = -0.5 * T.sum(1 + log_var - mean.pow(2) - log_var.exp()) / opt.batch_size\n",
    "    #Extract fake images corresponding to real images\n",
    "    x_recon = G(z, y_original)\n",
    "\n",
    "    #Extract latent_z corresponding to noise\n",
    "    z_p = T.randn(batch_size, opt.n_z)\n",
    "    z_p = get_cuda(z_p)\n",
    "\n",
    "    y_p = get_cuda(T.randint(0,10,(batch_size,)))\n",
    "    #Extract fake images corresponding to noise\n",
    "    x_noise = G(z_p, y_p)\n",
    "\n",
    "    #Compute D(x) for real and fake images along with their features\n",
    "    \n",
    "    label_original, fd_r = D(x_original, y_original)\n",
    "    label_recon, fd_f = D(x_recon, y_original)\n",
    "    label_noise, fd_p = D(x_noise, y_p)\n",
    "\n",
    "    #------------Discriminator training------------------\n",
    "    loss_D = F.binary_cross_entropy(label_original, y_real) + 0.5 * (F.binary_cross_entropy(label_recon, y_fake) + F.binary_cross_entropy(label_noise, y_fake))\n",
    "    D_trainer.zero_grad()\n",
    "    loss_D.backward(retain_graph = True)\n",
    "    D_trainer.step()\n",
    "\n",
    "    #------------Encoder & Generator/Decoder training--------------\n",
    "\n",
    "    #loss corresponding to -log(D(G(z_p))), has to look good, GAN loss\n",
    "    loss_GD = F.binary_cross_entropy(label_noise, y_real)\n",
    "    #pixel wise matching loss and discriminator's feature matching loss, reconstruct well\n",
    "    loss_G = 0.5 * (0.01*(x_recon - x_original).pow(2).sum() + (fd_f - fd_r.detach()).pow(2).sum()) / batch_size\n",
    "\n",
    "    E_trainer.zero_grad()\n",
    "    G_trainer.zero_grad()\n",
    "    (opt.w_kld*kld+opt.w_loss_g*loss_G+opt.w_loss_gd*loss_GD).backward()\n",
    "    E_trainer.step()\n",
    "    G_trainer.step()\n",
    "\n",
    "\n",
    "    return loss_D.item(), loss_G.item(), loss_GD.item(), kld.item()\n",
    "\n",
    "\n",
    "def test_batch(x_original, y_original):\n",
    "    batch_size = x_original.size(0)\n",
    "    y_real = get_cuda(T.ones(batch_size))\n",
    "    y_fake = get_cuda(T.zeros(batch_size))\n",
    "\n",
    "    #Extract latent_z corresponding to real images\n",
    "    z, mean, log_var = E(x_original, y_original)\n",
    "    kld = -0.5 * T.sum(1 + log_var - mean.pow(2) - log_var.exp()) / opt.batch_size\n",
    "    #Extract fake images corresponding to real images\n",
    "    x_recon = G(z, y_original)\n",
    "\n",
    "    #Extract latent_z corresponding to noise\n",
    "    z_p = T.randn(batch_size, opt.n_z)\n",
    "    z_p = get_cuda(z_p)\n",
    "\n",
    "    y_p = get_cuda(T.randint(0,10,(batch_size,)))\n",
    "    #Extract fake images corresponding to noise\n",
    "    x_noise = G(z_p, y_p)\n",
    "\n",
    "    #Compute D(x) for real and fake images along with their features\n",
    "    \n",
    "    label_original, fd_r = D(x_original, y_original)\n",
    "    label_recon, fd_f = D(x_recon, y_original)\n",
    "    label_noise, fd_p = D(x_noise, y_p)\n",
    "\n",
    "    #------------Discriminator training------------------\n",
    "    loss_D = F.binary_cross_entropy(label_original, y_real) + 0.5 * (F.binary_cross_entropy(label_recon, y_fake) + F.binary_cross_entropy(label_noise, y_fake))\n",
    "    \n",
    "    \n",
    "    acc_original = accuracy(y_original.cpu(), label_original.cpu())\n",
    "    acc_recon = accuracy(y_original.cpu(), label_original.cpu())\n",
    "    acc_noise = accuracy(y_original.cpu(), label_original.cpu())\n",
    "\n",
    "    #------------Encoder & Generator/Decoder training--------------\n",
    "\n",
    "    #loss corresponding to -log(D(G(z_p))), has to look good, GAN loss\n",
    "    loss_GD = F.binary_cross_entropy(label_noise, y_real)\n",
    "    #pixel wise matching loss and discriminator's feature matching loss, reconstruct well\n",
    "    loss_G = 0.5 * (0.01*(x_recon - x_original).pow(2).sum() + (fd_f - fd_r.detach()).pow(2).sum()) / batch_size\n",
    "\n",
    "\n",
    "    return loss_D.item(), loss_G.item(), loss_GD.item(), kld.item(), acc_noise,acc_original,acc_recon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_model_from_checkpoint():\n",
    "    global E, G, D, E_trainer, G_trainer, D_trainer\n",
    "    checkpoint = T.load(save_path)\n",
    "    E.load_state_dict(checkpoint['E_model'])\n",
    "    G.load_state_dict(checkpoint['G_model'])\n",
    "    D.load_state_dict(checkpoint['D_model'])\n",
    "    E_trainer.load_state_dict(checkpoint['E_trainer'])\n",
    "    G_trainer.load_state_dict(checkpoint['G_trainer'])\n",
    "    D_trainer.load_state_dict(checkpoint['D_trainer'])\n",
    "    return checkpoint['epoch']\n",
    "\n",
    "def generate_samples(img_name):\n",
    "    z_p = T.randn(10, opt.n_z)\n",
    "    y_p = get_cuda(T.arange(0,10))\n",
    "    z_p = get_cuda(z_p)\n",
    "    E.eval()\n",
    "    G.eval()\n",
    "    D.eval()\n",
    "    with T.autograd.no_grad():\n",
    "        x_p = G(z_p, y_p)\n",
    "    utils.save_image(x_p.cpu(), img_name, normalize=True, nrow=6)\n",
    "\n",
    "def accuracy(output, target):\n",
    "    \"\"\"Computes the accuracy for multiple binary predictions\"\"\"\n",
    "    pred = output >= 0.5\n",
    "    truth = target >= 0.5\n",
    "    acc = pred.eq(truth).sum().item() / target.numel()\n",
    "    return acc\n",
    "\n",
    "\n",
    "\n",
    "def loss_fn(recon_x, x, mean, log_var):\n",
    "        \"\"\"\n",
    "            recon_x : reconstructed x after being through VAE or CVAE\n",
    "            x : original x\n",
    "            mean : center of the gaussian in average\n",
    "            log_var : related to the standard deviation \n",
    "        \"\"\"\n",
    "        \n",
    "        recon_loss = T.nn.functional.binary_cross_entropy(recon_x.view(-1, 28*28), x.view(-1, 28*28))\n",
    "        KLD = -0.5 * T.sum(1 + log_var - mean.pow(2) - log_var.exp())/x.size(0)\n",
    "        return (100 * recon_loss +  KLD) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train VAE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vae = get_cuda(VAE(E,G))\n",
    "optimizer = T.optim.Adam(vae.parameters(), lr=0.0025)\n",
    "\n",
    "for epoch in range(10):\n",
    "\n",
    "\n",
    "    \"\"\"Do training\"\"\"\n",
    "\n",
    "    for iteration, (x, y) in enumerate(train_loader):\n",
    "        \"\"\"Send data to GPU\"\"\"\n",
    "        x, y = get_cuda(x), get_cuda(y)\n",
    "\n",
    "        if opt.conditional:\n",
    "            recon_x, mean, log_var, z = vae(x,y)\n",
    "        else:\n",
    "            recon_x, mean, log_var, z = vae(x)\n",
    "\n",
    "\n",
    "        \"\"\"Compute loss\"\"\"                           \n",
    "        loss = loss_fn(recon_x, x, mean, log_var)\n",
    "        diverge = -0.5 * T.sum(1 + log_var - mean.pow(2) - log_var.exp()) / opt.batch_size\n",
    "        \"\"\"Compute KL divergence and binary crossentropy\"\"\"\n",
    "        recon_loss = T.nn.functional.binary_cross_entropy(recon_x.view(-1, 28*28), x.view(-1, 28*28)) \n",
    "\n",
    "\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    print(\"loss VAE \",loss.item())\n",
    "generate_samples(\"data/results/pre.jpg\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train CVAEGAN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_epoch = 0\n",
    "if opt.resume_training:\n",
    "    start_epoch = load_model_from_checkpoint()\n",
    "\n",
    "for epoch in range(start_epoch, opt.epochs):\n",
    "    E.train()\n",
    "    G.train()\n",
    "    D.train()\n",
    "\n",
    "    T_loss_D = []\n",
    "    T_loss_G = []\n",
    "    T_loss_GD = []\n",
    "    T_loss_kld = []\n",
    "\n",
    "\n",
    "\n",
    "    for x, y in train_loader:\n",
    "        x = get_cuda(x)\n",
    "        loss_D, loss_G, loss_GD, loss_kld = train_batch(x,y)\n",
    "        T_loss_D.append(loss_D)\n",
    "        T_loss_G.append(loss_G)\n",
    "        T_loss_GD.append(loss_GD)\n",
    "        T_loss_kld.append(loss_kld)\n",
    "\n",
    "\n",
    "    T_loss_D = np.mean(T_loss_D)\n",
    "    T_loss_G = np.mean(T_loss_G)\n",
    "    T_loss_GD = np.mean(T_loss_GD)\n",
    "    T_loss_kld = np.mean(T_loss_kld)\n",
    "\n",
    "    print(\"epoch : \", epoch)\n",
    "\n",
    "    print(\"train -> loss_Discrim:\", \"%.4f\"%T_loss_D, \"loss_recon:\", \"%.4f\"%T_loss_G, \"loss_fool:\", \"%.4f\"%T_loss_GD, \"loss_kld:\", \"%.4f\"%T_loss_kld)\n",
    "\n",
    "    ################ DO the testing\n",
    "\n",
    "    Test_loss_D = []\n",
    "    Test_loss_G = []\n",
    "    Test_loss_GD = []\n",
    "    Test_loss_kld = []\n",
    "    acc_1 = []\n",
    "    acc_2 = []\n",
    "    acc_3 = []\n",
    "    for x, y in test_loader:\n",
    "        x = get_cuda(x)\n",
    "        with T.no_grad():\n",
    "            loss_D, loss_G, loss_GD, loss_kld, acc_noise,acc_original,acc_recon = test_batch(x,y)\n",
    "        Test_loss_D.append(loss_D)\n",
    "        Test_loss_G.append(loss_G)\n",
    "        Test_loss_GD.append(loss_GD)\n",
    "        Test_loss_kld.append(loss_kld)\n",
    "        acc_1.append(acc_noise)\n",
    "        acc_2.append(acc_original)\n",
    "        acc_3.append(acc_recon)\n",
    "\n",
    "    Test_loss_D = np.mean(Test_loss_D)\n",
    "    Test_loss_G = np.mean(Test_loss_G)\n",
    "    Test_loss_GD = np.mean(Test_loss_GD)\n",
    "    Test_loss_kld = np.mean(Test_loss_kld)\n",
    "\n",
    "    acc_n = np.mean(acc_1)\n",
    "    acc_o = np.mean(acc_2)\n",
    "    acc_r = np.mean(acc_3)\n",
    "\n",
    "    print(\"test  -> loss_Discrim:\", \"%.4f\"%Test_loss_D, \"loss_recon:\", \"%.4f\"%Test_loss_G, \"loss_fool:\", \"%.4f\"%Test_loss_GD,\n",
    "        \"loss_kld:\", \"%.4f\"%Test_loss_kld, \"accuracy noise \",int(acc_n*100),\" accuracy orig \", int(acc_o*100),\"accuracy recon \", int(acc_r*100))\n",
    "\n",
    "\n",
    "    generate_samples(\"data/results/%d.jpg\" % epoch)\n",
    "    T.save({\n",
    "        'epoch': epoch + 1,\n",
    "        \"E_model\": E.state_dict(),\n",
    "        \"G_model\": G.state_dict(),\n",
    "        \"D_model\": D.state_dict(),\n",
    "        'E_trainer': E_trainer.state_dict(),\n",
    "        'G_trainer': G_trainer.state_dict(),\n",
    "        'D_trainer': D_trainer.state_dict()\n",
    "    }, save_path)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
