{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "import os\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.nn.functional as F\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "from sklearn.utils import class_weight\n",
    "\n",
    "from load_moonboard import load_moonboard\n",
    "\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"]=\"PCI_BUS_ID\"\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MoonBoardDataset(Dataset):\n",
    "    \"\"\"Face Landmarks dataset.\"\"\"\n",
    "\n",
    "    def __init__(self, train = True):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            csv_file (string): Path to the csv file with annotations.\n",
    "            root_dir (string): Directory with all the images.\n",
    "            transform (callable, optional): Optional transform to be applied\n",
    "                on a sample.\n",
    "        \"\"\"\n",
    "        self.train = train\n",
    "        \n",
    "        (self.x_train, self.y_train), (self.x_test, self.y_test) = load_moonboard()\n",
    "        self.x_train = self.x_train.transpose(0,1,3,2 ).astype(float)\n",
    "        self.x_test = self.x_test.transpose(0,1,3,2 ).astype(float)\n",
    "        self.y_train = self.y_train.reshape(-1,1).astype(int)\n",
    "        self.y_test = self.y_test.reshape(-1,1).astype(int)\n",
    "        #self.y_train = np.eye(17)[self.y_train]\n",
    "        #self.y_test = np.eye(17)[self.y_test]\n",
    "\n",
    "    def __len__(self):\n",
    "        if self.train:\n",
    "            return len(self.x_train)\n",
    "        else:\n",
    "            return len(self.x_test)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        if self.train:\n",
    "            return self.x_train[idx], self.y_train[idx]\n",
    "        else:\n",
    "            return self.x_test[idx], self.y_test[idx]\n",
    "\n",
    "\n",
    "    # prepare data\n",
    "\n",
    "dataset = MoonBoardDataset(train = True)\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "    dataset,\n",
    "    batch_size=128, shuffle=True)\n",
    "    \n",
    "test_loader = torch.utils.data.DataLoader(\n",
    "    MoonBoardDataset(train = False),\n",
    "    batch_size=32, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 20, 2, 1)\n",
    "        self.conv2 = nn.Conv2d(20, 50, 2, 1)\n",
    "        self.fc1 = nn.Linear(3*11*18, 500)\n",
    "        self.fc2 = nn.Linear(500, 500)\n",
    "        self.fc3 = nn.Linear(500, 500)\n",
    "        self.fc4 = nn.Linear(500, 500)\n",
    "        self.fc5 = nn.Linear(500, 500)\n",
    "        self.fc6 = nn.Linear(500, 500)\n",
    "        self.fc7 = nn.Linear(500, 500)\n",
    "        self.fc8 = nn.Linear(500, 13)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.view(-1, 3*11*18)\n",
    "        x = self.fc1(x)\n",
    "        x = self.fc2(x)\n",
    "        x = self.fc3(x)\n",
    "        x = self.fc4(x)\n",
    "        #x = self.fc5(x)\n",
    "        #x = self.fc6(x)\n",
    "        #x = self.fc7(x)\n",
    "        x = self.fc8(x)\n",
    "        return x\n",
    "    \n",
    "    \n",
    "class ConvNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(ConvNet, self).__init__()\n",
    "        \n",
    "        self.plans = 128\n",
    "        self.conv1 = nn.Conv2d(3, self.plans, 1)\n",
    "        self.conv2 = nn.Conv2d(self.plans, self.plans*2, 5, 1, 3)\n",
    "        self.conv3 = nn.Conv2d(self.plans*2, self.plans*4, 5, 1, 3)\n",
    "        self.relu =  nn.ReLU(True)\n",
    "        self.pool = nn.MaxPool2d(2)\n",
    "        self.drop = nn.Dropout2d(0.2)\n",
    "        self.fc1 = nn.Linear(self.plans*4*6*4, 1000)\n",
    "        self.fc2 = nn.Linear(1000, 500)\n",
    "        self.fc3 = nn.Linear(500, 13)\n",
    "        self.softmax = nn.Softmax()\n",
    "        \n",
    "        self.bn1 = nn.BatchNorm2d(self.plans)\n",
    "        self.bn2 = nn.BatchNorm2d(self.plans*2)\n",
    "        self.bn3 = nn.BatchNorm2d(self.plans*4)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.bn1(x)\n",
    "        #x = self.pool(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.drop(x)\n",
    "        \n",
    "        x = self.conv2(x)\n",
    "        x = self.bn2(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.drop(x)\n",
    "        x = self.pool(x)\n",
    "        \n",
    "        x = self.conv3(x)\n",
    "        x = self.bn3(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.drop(x)\n",
    "        x = self.pool(x)\n",
    "        x = x.view(-1, self.plans*4*6*4)\n",
    "        x = self.fc1(x)\n",
    "        x = self.fc2(x)\n",
    "        x = self.drop(x)\n",
    "        x = self.fc3(x)\n",
    "        #x = nn.functional.log_softmax(x, dim= 1)\n",
    "        return x\n",
    "    \n",
    "class ResNet(nn.Module):\n",
    "    def __init__(self, block, num_blocks, num_classes=16):\n",
    "        super(ResNet, self).__init__()\n",
    "        self.in_planes = 64\n",
    "\n",
    "        self.conv1 = nn.Conv2d(3, 64, kernel_size=3, stride=1, padding=1, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(64)\n",
    "        self.layer1 = self._make_layer(block, 64, num_blocks[0], stride=1)\n",
    "        self.layer2 = self._make_layer(block, 128, num_blocks[1], stride=2)\n",
    "        self.layer3 = self._make_layer(block, 256, num_blocks[2], stride=2)\n",
    "        self.layer4 = self._make_layer(block, 512, num_blocks[3], stride=2)\n",
    "        self.linear = nn.Linear(512*block.expansion, num_classes)\n",
    "        \n",
    "        self.softmax = nn.Softmax(dim = 1)\n",
    "\n",
    "    def _make_layer(self, block, planes, num_blocks, stride):\n",
    "        strides = [stride] + [1]*(num_blocks-1)\n",
    "        layers = []\n",
    "        for stride in strides:\n",
    "            layers.append(block(self.in_planes, planes, stride))\n",
    "            self.in_planes = planes * block.expansion\n",
    "        return nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = F.relu(self.bn1(self.conv1(x)))\n",
    "        out = self.layer1(out)\n",
    "        out = self.layer2(out)\n",
    "        out = self.layer3(out)\n",
    "        out = self.layer4(out)\n",
    "        out = F.avg_pool2d(out, 4)\n",
    "        out = out.view(out.size(0), -1)\n",
    "        out = self.linear(out)\n",
    "        #out = self.softmax(out)\n",
    "        return out\n",
    "    \n",
    "class BasicBlock(nn.Module):\n",
    "    expansion = 1\n",
    "\n",
    "    def __init__(self, in_planes, planes, stride=1):\n",
    "        super(BasicBlock, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(in_planes, planes, kernel_size=3, stride=stride, padding=1, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(planes)\n",
    "        self.conv2 = nn.Conv2d(planes, planes, kernel_size=3, stride=1, padding=1, bias=False)\n",
    "        self.bn2 = nn.BatchNorm2d(planes)\n",
    "\n",
    "        self.shortcut = nn.Sequential()\n",
    "        if stride != 1 or in_planes != self.expansion*planes:\n",
    "            self.shortcut = nn.Sequential(\n",
    "                nn.Conv2d(in_planes, self.expansion*planes, kernel_size=1, stride=stride, bias=False),\n",
    "                nn.BatchNorm2d(self.expansion*planes)\n",
    "            )\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = F.relu(self.bn1(self.conv1(x)))\n",
    "        out = self.bn2(self.conv2(out))\n",
    "        out += self.shortcut(x)\n",
    "        out = F.relu(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(0)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)\n",
    "\n",
    "\n",
    "model = ResNet(BasicBlock, [1,1,1,1]).to(device)\n",
    "#model = ConvNet().to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = optim.Adam(model.parameters(), weight_decay=0.01)\n",
    "#optimizer = optim.SGD(model.parameters(), lr=0.001, momentum=0.7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "def acc(model, test=True):\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        if test:\n",
    "            loader = test_loader\n",
    "        else:\n",
    "            loader = train_loader\n",
    "        for data in loader:\n",
    "            images, labels = data\n",
    "            outputs = model(images.to(device, dtype=torch.float32))\n",
    "\n",
    "            outputs = outputs.view((-1,16))   \n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "\n",
    "            \n",
    "            labels = labels.view((-1))\n",
    "            total += labels.size(0)\n",
    "            correct += (abs(predicted - labels.to(device, dtype=torch.long))<1).sum().item()\n",
    "\n",
    "    if test:\n",
    "        st = \"test\"\n",
    "    else:\n",
    "        st = \"train\"\n",
    "    print('Accuracy of the network on ',st,' images: %f' % (\n",
    "        100. * float(correct) / float(total)))\n",
    "    return 100. * float(correct) / float(total)\n",
    "    \n",
    "    \n",
    "def acc_class(model):\n",
    "    class_correct = list(0. for i in range(18))\n",
    "    class_total = list(0. for i in range(18))\n",
    "    with torch.no_grad():\n",
    "        for data in test_loader:\n",
    "            images, labels = data\n",
    "            outputs = model(images.to(device, dtype=torch.float32))\n",
    "            outputs = outputs.view((-1,16))\n",
    "\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            \n",
    "\n",
    "            labels = labels.view((-1))\n",
    "            c = (predicted == labels.to(device, dtype=torch.long))\n",
    "            for i in range(len(labels)):\n",
    "                label = labels[i].item()\n",
    "                class_correct[label] += c[i].item()\n",
    "                class_total[label] += 1\n",
    "\n",
    "\n",
    "    for i in range(0,18):\n",
    "        if class_total[i] != 0:\n",
    "            print('Accuracy of %5s : %2d %%' % (\n",
    "                i, 100. * class_correct[i] // class_total[i]))\n",
    "    \n",
    "\n",
    "def train(model, device, train_loader, optimizer, epoch):\n",
    "    model.train()\n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "        data, target = data.to(device, dtype=torch.float32), target.to(device, dtype=torch.long)\n",
    "        optimizer.zero_grad()\n",
    "        output = model(data)\n",
    "        target = target.view((-1))\n",
    "        loss = F.cross_entropy(output, target)#, weight=torch.tensor(weights).to(device, dtype = torch.float))\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        if batch_idx % 10 == 0:\n",
    "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
    "                epoch, batch_idx * len(data), len(train_loader.dataset),\n",
    "                100. * batch_idx // len(train_loader), loss.item()))\n",
    "    \n",
    "        \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "best = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 0 [0/4129 (0%)]\tLoss: 1.104287\n",
      "Train Epoch: 0 [1280/4129 (30%)]\tLoss: 0.330170\n",
      "Train Epoch: 0 [2560/4129 (60%)]\tLoss: 0.189032\n",
      "Train Epoch: 0 [3840/4129 (90%)]\tLoss: 0.192738\n",
      "Accuracy of the network on  test  images: 89.157793\n",
      "Accuracy of the network on  train  images: 96.657786\n",
      "Train Epoch: 1 [0/4129 (0%)]\tLoss: 0.156609\n",
      "Train Epoch: 1 [1280/4129 (30%)]\tLoss: 0.153735\n",
      "Train Epoch: 1 [2560/4129 (60%)]\tLoss: 0.077351\n",
      "Train Epoch: 1 [3840/4129 (90%)]\tLoss: 0.116990\n",
      "Accuracy of the network on  test  images: 90.416263\n",
      "Accuracy of the network on  train  images: 98.813272\n",
      "Train Epoch: 2 [0/4129 (0%)]\tLoss: 0.049157\n",
      "Train Epoch: 2 [1280/4129 (30%)]\tLoss: 0.102245\n",
      "Train Epoch: 2 [2560/4129 (60%)]\tLoss: 0.024890\n",
      "Train Epoch: 2 [3840/4129 (90%)]\tLoss: 0.022466\n",
      "Accuracy of the network on  test  images: 91.674734\n",
      "Accuracy of the network on  train  images: 99.515621\n",
      "Train Epoch: 3 [0/4129 (0%)]\tLoss: 0.074565\n",
      "Train Epoch: 3 [1280/4129 (30%)]\tLoss: 0.026148\n",
      "Train Epoch: 3 [2560/4129 (60%)]\tLoss: 0.061593\n",
      "Train Epoch: 3 [3840/4129 (90%)]\tLoss: 0.029338\n",
      "Accuracy of the network on  test  images: 92.352372\n",
      "Accuracy of the network on  train  images: 99.636716\n",
      "Train Epoch: 4 [0/4129 (0%)]\tLoss: 0.014392\n",
      "Train Epoch: 4 [1280/4129 (30%)]\tLoss: 0.042486\n",
      "Train Epoch: 4 [2560/4129 (60%)]\tLoss: 0.020802\n",
      "Train Epoch: 4 [3840/4129 (90%)]\tLoss: 0.026097\n",
      "Accuracy of the network on  test  images: 92.158761\n",
      "Accuracy of the network on  train  images: 99.757811\n",
      "Train Epoch: 5 [0/4129 (0%)]\tLoss: 0.014355\n",
      "Train Epoch: 5 [1280/4129 (30%)]\tLoss: 0.036419\n",
      "Train Epoch: 5 [2560/4129 (60%)]\tLoss: 0.020848\n",
      "Train Epoch: 5 [3840/4129 (90%)]\tLoss: 0.016641\n",
      "Accuracy of the network on  test  images: 91.868345\n",
      "Accuracy of the network on  train  images: 99.806248\n",
      "Train Epoch: 6 [0/4129 (0%)]\tLoss: 0.023931\n",
      "Train Epoch: 6 [1280/4129 (30%)]\tLoss: 0.016156\n",
      "Train Epoch: 6 [2560/4129 (60%)]\tLoss: 0.011995\n",
      "Train Epoch: 6 [3840/4129 (90%)]\tLoss: 0.028723\n",
      "Accuracy of the network on  test  images: 92.352372\n",
      "Accuracy of the network on  train  images: 99.854686\n",
      "Train Epoch: 7 [0/4129 (0%)]\tLoss: 0.019145\n",
      "Train Epoch: 7 [1280/4129 (30%)]\tLoss: 0.016970\n",
      "Train Epoch: 7 [2560/4129 (60%)]\tLoss: 0.033842\n",
      "Train Epoch: 7 [3840/4129 (90%)]\tLoss: 0.012201\n",
      "Accuracy of the network on  test  images: 92.739593\n",
      "Accuracy of the network on  train  images: 99.806248\n",
      "Train Epoch: 8 [0/4129 (0%)]\tLoss: 0.012708\n",
      "Train Epoch: 8 [1280/4129 (30%)]\tLoss: 0.017775\n",
      "Train Epoch: 8 [2560/4129 (60%)]\tLoss: 0.017527\n",
      "Train Epoch: 8 [3840/4129 (90%)]\tLoss: 0.022864\n",
      "Accuracy of the network on  test  images: 93.030010\n",
      "Accuracy of the network on  train  images: 99.782030\n",
      "Train Epoch: 9 [0/4129 (0%)]\tLoss: 0.020357\n",
      "Train Epoch: 9 [1280/4129 (30%)]\tLoss: 0.012366\n",
      "Train Epoch: 9 [2560/4129 (60%)]\tLoss: 0.057958\n",
      "Train Epoch: 9 [3840/4129 (90%)]\tLoss: 0.039586\n",
      "Accuracy of the network on  test  images: 92.255566\n",
      "Accuracy of the network on  train  images: 99.830467\n",
      "Train Epoch: 10 [0/4129 (0%)]\tLoss: 0.015346\n",
      "Train Epoch: 10 [1280/4129 (30%)]\tLoss: 0.017684\n",
      "Train Epoch: 10 [2560/4129 (60%)]\tLoss: 0.027643\n",
      "Train Epoch: 10 [3840/4129 (90%)]\tLoss: 0.043134\n",
      "Accuracy of the network on  test  images: 92.352372\n",
      "Accuracy of the network on  train  images: 99.709373\n",
      "Train Epoch: 11 [0/4129 (0%)]\tLoss: 0.013158\n",
      "Train Epoch: 11 [1280/4129 (30%)]\tLoss: 0.030147\n",
      "Train Epoch: 11 [2560/4129 (60%)]\tLoss: 0.018316\n",
      "Train Epoch: 11 [3840/4129 (90%)]\tLoss: 0.024715\n",
      "Accuracy of the network on  test  images: 92.739593\n",
      "Accuracy of the network on  train  images: 99.709373\n",
      "Train Epoch: 12 [0/4129 (0%)]\tLoss: 0.016263\n",
      "Train Epoch: 12 [1280/4129 (30%)]\tLoss: 0.053951\n",
      "Train Epoch: 12 [2560/4129 (60%)]\tLoss: 0.122657\n",
      "Train Epoch: 12 [3840/4129 (90%)]\tLoss: 0.084864\n",
      "Accuracy of the network on  test  images: 90.416263\n",
      "Accuracy of the network on  train  images: 97.965609\n",
      "Train Epoch: 13 [0/4129 (0%)]\tLoss: 0.041531\n",
      "Train Epoch: 13 [1280/4129 (30%)]\tLoss: 0.173225\n",
      "Train Epoch: 13 [2560/4129 (60%)]\tLoss: 0.083546\n",
      "Train Epoch: 13 [3840/4129 (90%)]\tLoss: 0.082763\n",
      "Accuracy of the network on  test  images: 91.093901\n",
      "Accuracy of the network on  train  images: 98.861710\n",
      "Train Epoch: 14 [0/4129 (0%)]\tLoss: 0.073261\n",
      "Train Epoch: 14 [1280/4129 (30%)]\tLoss: 0.041672\n",
      "Train Epoch: 14 [2560/4129 (60%)]\tLoss: 0.103864\n",
      "Train Epoch: 14 [3840/4129 (90%)]\tLoss: 0.062513\n",
      "Accuracy of the network on  test  images: 91.771539\n",
      "Accuracy of the network on  train  images: 99.636716\n",
      "Train Epoch: 15 [0/4129 (0%)]\tLoss: 0.024312\n",
      "Train Epoch: 15 [1280/4129 (30%)]\tLoss: 0.025070\n",
      "Train Epoch: 15 [2560/4129 (60%)]\tLoss: 0.033383\n",
      "Train Epoch: 15 [3840/4129 (90%)]\tLoss: 0.026450\n",
      "Accuracy of the network on  test  images: 91.577928\n",
      "Accuracy of the network on  train  images: 99.854686\n",
      "Train Epoch: 16 [0/4129 (0%)]\tLoss: 0.013855\n",
      "Train Epoch: 16 [1280/4129 (30%)]\tLoss: 0.016882\n",
      "Train Epoch: 16 [2560/4129 (60%)]\tLoss: 0.017060\n",
      "Train Epoch: 16 [3840/4129 (90%)]\tLoss: 0.017812\n",
      "Accuracy of the network on  test  images: 93.030010\n",
      "Accuracy of the network on  train  images: 99.782030\n",
      "Train Epoch: 17 [0/4129 (0%)]\tLoss: 0.018537\n",
      "Train Epoch: 17 [1280/4129 (30%)]\tLoss: 0.028273\n",
      "Train Epoch: 17 [2560/4129 (60%)]\tLoss: 0.014309\n",
      "Train Epoch: 17 [3840/4129 (90%)]\tLoss: 0.024553\n",
      "Accuracy of the network on  test  images: 92.739593\n",
      "Accuracy of the network on  train  images: 99.806248\n",
      "Train Epoch: 18 [0/4129 (0%)]\tLoss: 0.016508\n",
      "Train Epoch: 18 [1280/4129 (30%)]\tLoss: 0.021506\n",
      "Train Epoch: 18 [2560/4129 (60%)]\tLoss: 0.012226\n",
      "Train Epoch: 18 [3840/4129 (90%)]\tLoss: 0.056452\n",
      "Accuracy of the network on  test  images: 93.030010\n",
      "Accuracy of the network on  train  images: 99.782030\n",
      "Train Epoch: 19 [0/4129 (0%)]\tLoss: 0.011420\n",
      "Train Epoch: 19 [1280/4129 (30%)]\tLoss: 0.012964\n",
      "Train Epoch: 19 [2560/4129 (60%)]\tLoss: 0.029789\n",
      "Train Epoch: 19 [3840/4129 (90%)]\tLoss: 0.012046\n",
      "Accuracy of the network on  test  images: 92.642788\n",
      "Accuracy of the network on  train  images: 99.854686\n",
      "Train Epoch: 20 [0/4129 (0%)]\tLoss: 0.015759\n",
      "Train Epoch: 20 [1280/4129 (30%)]\tLoss: 0.026765\n",
      "Train Epoch: 20 [2560/4129 (60%)]\tLoss: 0.014725\n",
      "Train Epoch: 20 [3840/4129 (90%)]\tLoss: 0.011625\n",
      "Accuracy of the network on  test  images: 92.642788\n",
      "Accuracy of the network on  train  images: 99.757811\n",
      "Train Epoch: 21 [0/4129 (0%)]\tLoss: 0.011329\n",
      "Train Epoch: 21 [1280/4129 (30%)]\tLoss: 0.018535\n",
      "Train Epoch: 21 [2560/4129 (60%)]\tLoss: 0.017004\n",
      "Train Epoch: 21 [3840/4129 (90%)]\tLoss: 0.011497\n",
      "Accuracy of the network on  test  images: 92.836399\n",
      "Accuracy of the network on  train  images: 99.830467\n",
      "Train Epoch: 22 [0/4129 (0%)]\tLoss: 0.012889\n",
      "Train Epoch: 22 [1280/4129 (30%)]\tLoss: 0.030972\n",
      "Train Epoch: 22 [2560/4129 (60%)]\tLoss: 0.013139\n",
      "Train Epoch: 22 [3840/4129 (90%)]\tLoss: 0.011956\n",
      "Accuracy of the network on  test  images: 92.933204\n",
      "Accuracy of the network on  train  images: 99.854686\n",
      "Train Epoch: 23 [0/4129 (0%)]\tLoss: 0.019112\n",
      "Train Epoch: 23 [1280/4129 (30%)]\tLoss: 0.014919\n",
      "Train Epoch: 23 [2560/4129 (60%)]\tLoss: 0.022738\n",
      "Train Epoch: 23 [3840/4129 (90%)]\tLoss: 0.011445\n",
      "Accuracy of the network on  test  images: 92.933204\n",
      "Accuracy of the network on  train  images: 99.878905\n",
      "Train Epoch: 24 [0/4129 (0%)]\tLoss: 0.011336\n",
      "Train Epoch: 24 [1280/4129 (30%)]\tLoss: 0.012260\n",
      "Train Epoch: 24 [2560/4129 (60%)]\tLoss: 0.010702\n",
      "Train Epoch: 24 [3840/4129 (90%)]\tLoss: 0.034146\n",
      "Accuracy of the network on  test  images: 92.642788\n",
      "Accuracy of the network on  train  images: 99.830467\n",
      "Train Epoch: 25 [0/4129 (0%)]\tLoss: 0.012460\n",
      "Train Epoch: 25 [1280/4129 (30%)]\tLoss: 0.017516\n",
      "Train Epoch: 25 [2560/4129 (60%)]\tLoss: 0.015679\n",
      "Train Epoch: 25 [3840/4129 (90%)]\tLoss: 0.031416\n",
      "Accuracy of the network on  test  images: 92.255566\n",
      "Accuracy of the network on  train  images: 99.733592\n",
      "Train Epoch: 26 [0/4129 (0%)]\tLoss: 0.014885\n",
      "Train Epoch: 26 [1280/4129 (30%)]\tLoss: 0.021130\n",
      "Train Epoch: 26 [2560/4129 (60%)]\tLoss: 0.012500\n",
      "Train Epoch: 26 [3840/4129 (90%)]\tLoss: 0.037441\n",
      "Accuracy of the network on  test  images: 91.190707\n",
      "Accuracy of the network on  train  images: 99.273432\n",
      "Train Epoch: 27 [0/4129 (0%)]\tLoss: 0.063917\n",
      "Train Epoch: 27 [1280/4129 (30%)]\tLoss: 0.014098\n",
      "Train Epoch: 27 [2560/4129 (60%)]\tLoss: 0.032202\n",
      "Train Epoch: 27 [3840/4129 (90%)]\tLoss: 0.078150\n",
      "Accuracy of the network on  test  images: 89.932236\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the network on  train  images: 98.232017\n",
      "Train Epoch: 28 [0/4129 (0%)]\tLoss: 0.087750\n",
      "Train Epoch: 28 [1280/4129 (30%)]\tLoss: 0.074251\n",
      "Train Epoch: 28 [2560/4129 (60%)]\tLoss: 0.110028\n",
      "Train Epoch: 28 [3840/4129 (90%)]\tLoss: 0.143294\n",
      "Accuracy of the network on  test  images: 90.029042\n",
      "Accuracy of the network on  train  images: 98.038266\n",
      "Train Epoch: 29 [0/4129 (0%)]\tLoss: 0.069195\n",
      "Train Epoch: 29 [1280/4129 (30%)]\tLoss: 0.119528\n",
      "Train Epoch: 29 [2560/4129 (60%)]\tLoss: 0.072671\n",
      "Train Epoch: 29 [3840/4129 (90%)]\tLoss: 0.050498\n",
      "Accuracy of the network on  test  images: 91.093901\n",
      "Accuracy of the network on  train  images: 99.224994\n",
      "Accuracy of     0 : 94 %\n",
      "Accuracy of     1 : 73 %\n",
      "Accuracy of     2 : 48 %\n"
     ]
    }
   ],
   "source": [
    "for i in range(30):\n",
    "    train(model, device, train_loader, optimizer, i)\n",
    "    ac = acc(model)\n",
    "    acc(model, test= False)\n",
    "    if ac > best:\n",
    "        best = ac\n",
    "        torch.save(model,\"inception_model.trch\")\n",
    "    \n",
    "acc_class(model)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "(x_train, y_train), (x_test, y_test) = load_moonboard()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[3346, 608, 175, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[180.96010919693754, 6850.152179889726, 27015.957287845446, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[845, 144, 44, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "5162\n"
     ]
    }
   ],
   "source": [
    "count_train = [0]*16\n",
    "for label in y_train:\n",
    "    count_train[label] +=1\n",
    "print(count_train)\n",
    "s = sum(count_train)\n",
    "weights = [s**6/(i*(1000**5))-1300  if i != 0 else 0 for i in count_train]\n",
    "print(weights)\n",
    "\n",
    "count_test = [0]*16\n",
    "for label in y_test:\n",
    "    count_test[label] +=1\n",
    "print(count_test)\n",
    "\n",
    "print(sum(count_test) + sum(count_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
