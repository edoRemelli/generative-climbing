{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import os\n",
    "import random\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.nn.parallel\n",
    "import torch.backends.cudnn as cudnn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torchvision.datasets as dset\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.utils as vutils\n",
    "from scipy.stats import entropy\n",
    "import numpy as np\n",
    "\n",
    "from load_moonboard import load_moonboard\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "parser = argparse.ArgumentParser()\n",
    "parser.add_argument('--workers', type=int, help='number of data loading workers', default=0)\n",
    "parser.add_argument('--batchSize', type=int, default=512, help='input batch size')\n",
    "parser.add_argument('--imageSize', type=int, default=512, help='the height / width of the input image to network')\n",
    "parser.add_argument('--nz', type=int, default=200, help='size of the latent z vector')\n",
    "parser.add_argument('--ngf', type=int, default=128)\n",
    "parser.add_argument('--ndf', type=int, default=32)\n",
    "parser.add_argument('--niter', type=int, default=70, help='number of epochs to train for')\n",
    "parser.add_argument('--lr', type=float, default=0.0001, help='learning rate, default=0.0002')\n",
    "parser.add_argument('--beta1', type=float, default=0.5, help='beta1 for adam. default=0.5')\n",
    "parser.add_argument('--cuda', action='store_true', help='enables cuda')\n",
    "parser.add_argument('--ngpu', type=int, default=1, help='number of GPUs to use')\n",
    "parser.add_argument('--netG', default='', help=\"path to netG (to continue training)\")\n",
    "parser.add_argument('--netD', default='', help=\"path to netD (to continue training)\")\n",
    "parser.add_argument('--outf', default='.', help='folder to output images and model checkpoints')\n",
    "parser.add_argument('--manualSeed', type=int, help='manual seed')\n",
    "\n",
    "opt = parser.parse_args([])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Seed:  1957\n"
     ]
    }
   ],
   "source": [
    "if opt.manualSeed is None:\n",
    "    opt.manualSeed = random.randint(1, 10000)\n",
    "print(\"Random Seed: \", opt.manualSeed)\n",
    "random.seed(opt.manualSeed)\n",
    "torch.manual_seed(opt.manualSeed)\n",
    "\n",
    "cudnn.benchmark = True\n",
    "\n",
    "\n",
    "\"\"\"Load the dataset\"\"\"\n",
    "class MoonBoardDataset(Dataset):\n",
    "    \"\"\"Face Landmarks dataset.\"\"\"\n",
    "\n",
    "    def __init__(self, train = True):\n",
    "        self.train = train\n",
    "\n",
    "        (self.x_train, self.y_train), (self.x_test, self.y_test) = load_moonboard()\n",
    "        self.x_train = self.x_train.transpose(0,1,3,2 ).astype(float)\n",
    "        self.x_test = self.x_test.transpose(0,1,3,2 ).astype(float)\n",
    "        self.y_train = self.y_train.reshape(-1,1).astype(int)\n",
    "        self.y_test = self.y_test.reshape(-1,1).astype(int)\n",
    "\n",
    "    def __len__(self):\n",
    "        if self.train:\n",
    "            return len(self.x_train)\n",
    "        else:\n",
    "            return len(self.x_test)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        if self.train:\n",
    "            return self.x_train[idx], self.y_train[idx]\n",
    "        else:\n",
    "            return self.x_test[idx], self.y_test[idx]\n",
    "\n",
    "dataset = MoonBoardDataset(train = True)\n",
    "\n",
    "dataloader = torch.utils.data.DataLoader(dataset, batch_size=opt.batchSize,\n",
    "                                         shuffle=True, num_workers=int(opt.workers))\n",
    "\n",
    "nc = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "print(torch.cuda.is_available())\n",
    "device = torch.device(\"cuda\")\n",
    "ngpu = int(opt.ngpu)\n",
    "nz = int(opt.nz)\n",
    "ngf = int(opt.ngf)\n",
    "ndf = int(opt.ndf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Discriminator(\n",
       "  (leakyRelu): LeakyReLU(negative_slope=0.2, inplace)\n",
       "  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (conv1): Conv2d(3, 32, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "  (conv2): Conv2d(32, 64, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "  (conv3): Conv2d(64, 1, kernel_size=(5, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "  (sigmoid): Sigmoid()\n",
       ")"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# custom weights initialization called on netG and netD\n",
    "def weights_init(m):\n",
    "    classname = m.__class__.__name__\n",
    "    if classname.find('Conv') != -1:\n",
    "        m.weight.data.normal_(0.0, 0.02)\n",
    "    elif classname.find('BatchNorm') != -1:\n",
    "        m.weight.data.normal_(1.0, 0.02)\n",
    "        m.bias.data.fill_(0)\n",
    "\n",
    "\n",
    "class Generator(nn.Module):\n",
    "    def __init__(self, ngpu):\n",
    "        super(Generator, self).__init__()\n",
    "        self.ngpu = ngpu\n",
    "        self.convtrans1 = nn.ConvTranspose2d(nz, ngf * 4, (5,4), 2, 0, bias=False)\n",
    "        self.convtrans2 = nn.ConvTranspose2d(ngf * 4, ngf * 2, (5,4), 2, 2, bias=False)\n",
    "        self.convtrans3 = nn.ConvTranspose2d(    ngf * 2,      nc, (4,3), 2, 1, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(ngf * 4)\n",
    "        self.bn2 = nn.BatchNorm2d(ngf * 2)\n",
    "        self.tanh = nn.Tanh()\n",
    "        self.relu = nn.ReLU()\n",
    "        \n",
    "   \n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.convtrans1(x)\n",
    "        x = self.bn1(x)\n",
    "        x = self.relu(x)\n",
    "        \n",
    "        x = self.convtrans2(x)\n",
    "        x = self.bn2(x)\n",
    "        x = self.relu(x)\n",
    "        \n",
    "        x = self.convtrans3(x)\n",
    "        x = self.relu(x)\n",
    "        \n",
    "        x = self.tanh(x)\n",
    "        \n",
    "        return x\n",
    "\n",
    "\n",
    "netG = Generator(ngpu).to(device)\n",
    "netG.apply(weights_init)\n",
    "\n",
    "\n",
    "class Discriminator(nn.Module):\n",
    "    def __init__(self, ngpu):\n",
    "        super(Discriminator, self).__init__()\n",
    "        self.ngpu = ngpu\n",
    "        \n",
    "        self.leakyRelu = nn.LeakyReLU(0.2, inplace=True)\n",
    "        self.bn1 = nn.BatchNorm2d(ndf * 2)\n",
    "        self.bn2 = nn.BatchNorm2d(ndf * 4)\n",
    "        self.bn3 = nn.BatchNorm2d(ndf * 8)\n",
    "        self.conv1 = nn.Conv2d(nc, ndf, 4, 2, 1, bias=False)\n",
    "        self.conv2 = nn.Conv2d(ndf, ndf * 2, 4, 2, 1, bias=False)\n",
    "        self.conv3 = nn.Conv2d(ndf * 2, 1, (5,4), 2, 1, bias=False)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.leakyRelu(x)\n",
    "        \n",
    "        x = self.conv2(x)\n",
    "        x = self.bn1(x)\n",
    "        x = self.leakyRelu(x)\n",
    "        \n",
    "        x = self.conv3(x)\n",
    "        \n",
    "        return x.view(-1, 1).squeeze(1)\n",
    "\n",
    "\n",
    "netD = Discriminator(ngpu).to(device)\n",
    "netD.apply(weights_init)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResNet(nn.Module):\n",
    "    def __init__(self, block, num_blocks, num_classes=13):\n",
    "        super(ResNet, self).__init__()\n",
    "        self.in_planes = 64\n",
    "\n",
    "        self.conv1 = nn.Conv2d(3, 64, kernel_size=3, stride=1, padding=1, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(64)\n",
    "        self.layer1 = self._make_layer(block, 64, num_blocks[0], stride=1)\n",
    "        self.layer2 = self._make_layer(block, 128, num_blocks[1], stride=2)\n",
    "        self.layer3 = self._make_layer(block, 256, num_blocks[2], stride=2)\n",
    "        self.layer4 = self._make_layer(block, 512, num_blocks[3], stride=2)\n",
    "        self.linear = nn.Linear(512*block.expansion, num_classes)\n",
    "\n",
    "    def _make_layer(self, block, planes, num_blocks, stride):\n",
    "        strides = [stride] + [1]*(num_blocks-1)\n",
    "        layers = []\n",
    "        for stride in strides:\n",
    "            layers.append(block(self.in_planes, planes, stride))\n",
    "            self.in_planes = planes * block.expansion\n",
    "        return nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = F.relu(self.bn1(self.conv1(x)))\n",
    "        out = self.layer1(out)\n",
    "        out = self.layer2(out)\n",
    "        out = self.layer3(out)\n",
    "        out = self.layer4(out)\n",
    "        out = F.avg_pool2d(out, 4)\n",
    "        out = out.view(out.size(0), -1)\n",
    "        out = self.linear(out)\n",
    "        return out\n",
    "    \n",
    "class BasicBlock(nn.Module):\n",
    "    expansion = 1\n",
    "\n",
    "    def __init__(self, in_planes, planes, stride=1):\n",
    "        super(BasicBlock, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(in_planes, planes, kernel_size=3, stride=stride, padding=1, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(planes)\n",
    "        self.conv2 = nn.Conv2d(planes, planes, kernel_size=3, stride=1, padding=1, bias=False)\n",
    "        self.bn2 = nn.BatchNorm2d(planes)\n",
    "\n",
    "        self.shortcut = nn.Sequential()\n",
    "        if stride != 1 or in_planes != self.expansion*planes:\n",
    "            self.shortcut = nn.Sequential(\n",
    "                nn.Conv2d(in_planes, self.expansion*planes, kernel_size=1, stride=stride, bias=False),\n",
    "                nn.BatchNorm2d(self.expansion*planes)\n",
    "            )\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = F.relu(self.bn1(self.conv1(x)))\n",
    "        out = self.bn2(self.conv2(out))\n",
    "        out += self.shortcut(x)\n",
    "        out = F.relu(out)\n",
    "        return out\n",
    "    \n",
    "def inception_score(imgs, inception_model, splits = 1):\n",
    "\n",
    "    N = len(imgs)    \n",
    "    split_scores = []\n",
    "    \n",
    "    preds = nn.Softmax(dim = 1)(inception_model(imgs))\n",
    "    \n",
    "    for k in range(splits):\n",
    "        part = preds[k * (N // splits): (k+1) * (N // splits), :]\n",
    "        py = torch.mean(part, 0)\n",
    "        scores = []\n",
    "        for i in range(part.shape[0]):\n",
    "            pyx = part[i, :]\n",
    "            scores.append(entropy(pyx.cpu().detach().numpy(), py.cpu().detach().numpy()))\n",
    "        split_scores.append(np.exp(np.mean(scores)))\n",
    "\n",
    "    return np.mean(split_scores), np.std(split_scores)\n",
    "    \n",
    "inception_model = torch.load(\"inception_model.trch\").to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.MSELoss()\n",
    "\n",
    "fixed_noise = torch.randn(opt.batchSize, nz, 1, 1, device=device)\n",
    "real_label = 1\n",
    "fake_label = 0\n",
    "\n",
    "# setup optimizer\n",
    "optimizerD = optim.Adam(netD.parameters(), lr=opt.lr, betas=(opt.beta1, 0.999))\n",
    "optimizerG = optim.Adam(netG.parameters(), lr=opt.lr, betas=(opt.beta1, 0.999))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inception score  (1.1749289, 0.0)\n",
      "[0/70][8/9] Loss_D: 1.4462 Loss_G: 0.0061 D(x): 0.0632 D(G(z)): 0.7051 / 1.0392\n",
      "Inception score  (1.1419706, 0.0)\n",
      "[1/70][8/9] Loss_D: 0.9726 Loss_G: 0.0100 D(x): 0.2874 D(G(z)): 0.5959 / 0.9875\n",
      "Inception score  (1.1401275, 0.0)\n",
      "[2/70][8/9] Loss_D: 0.7246 Loss_G: 0.0093 D(x): 0.4497 D(G(z)): 0.5486 / 0.9696\n",
      "Inception score  (1.1853774, 0.0)\n",
      "[3/70][8/9] Loss_D: 0.7532 Loss_G: 0.0126 D(x): 0.4570 D(G(z)): 0.5998 / 0.9283\n",
      "Inception score  (1.1885377, 0.0)\n",
      "[4/70][8/9] Loss_D: 0.7423 Loss_G: 0.0418 D(x): 0.4089 D(G(z)): 0.5572 / 0.8175\n",
      "Inception score  (1.2034277, 0.0)\n",
      "[5/70][8/9] Loss_D: 0.7162 Loss_G: 0.1172 D(x): 0.3501 D(G(z)): 0.5010 / 0.6651\n",
      "Inception score  (1.2087543, 0.0)\n",
      "[6/70][8/9] Loss_D: 0.5764 Loss_G: 0.1955 D(x): 0.4129 D(G(z)): 0.4215 / 0.5654\n",
      "Inception score  (1.2083981, 0.0)\n",
      "[7/70][8/9] Loss_D: 0.4933 Loss_G: 0.1998 D(x): 0.4701 D(G(z)): 0.4048 / 0.5653\n",
      "Inception score  (1.1869347, 0.0)\n",
      "[8/70][8/9] Loss_D: 0.4777 Loss_G: 0.3402 D(x): 0.4415 D(G(z)): 0.3797 / 0.4207\n",
      "Inception score  (1.1783307, 0.0)\n",
      "[9/70][8/9] Loss_D: 0.5372 Loss_G: 0.1733 D(x): 0.5128 D(G(z)): 0.5059 / 0.5923\n",
      "Inception score  (1.1576824, 0.0)\n",
      "[10/70][8/9] Loss_D: 0.3500 Loss_G: 0.4352 D(x): 0.4974 D(G(z)): 0.2325 / 0.3441\n",
      "Inception score  (1.1827787, 0.0)\n",
      "[11/70][8/9] Loss_D: 0.3872 Loss_G: 0.3494 D(x): 0.6007 D(G(z)): 0.4246 / 0.4121\n",
      "Inception score  (1.2315316, 0.0)\n",
      "[12/70][8/9] Loss_D: 0.3433 Loss_G: 0.1754 D(x): 0.5893 D(G(z)): 0.3165 / 0.6305\n",
      "Inception score  (1.1849927, 0.0)\n",
      "[13/70][8/9] Loss_D: 0.7767 Loss_G: 0.0909 D(x): 0.5840 D(G(z)): 0.7329 / 0.7044\n",
      "Inception score  (1.097283, 0.0)\n",
      "[14/70][8/9] Loss_D: 0.5118 Loss_G: 0.3224 D(x): 0.5077 D(G(z)): 0.4613 / 0.4380\n",
      "Inception score  (1.1891584, 0.0)\n",
      "[15/70][8/9] Loss_D: 0.5025 Loss_G: 0.3320 D(x): 0.4725 D(G(z)): 0.4269 / 0.4271\n",
      "Inception score  (1.1908771, 0.0)\n",
      "[16/70][8/9] Loss_D: 0.4617 Loss_G: 0.2744 D(x): 0.4739 D(G(z)): 0.3795 / 0.4826\n",
      "Inception score  (1.1861837, 0.0)\n",
      "[17/70][8/9] Loss_D: 0.4244 Loss_G: 0.2814 D(x): 0.5676 D(G(z)): 0.4398 / 0.4759\n",
      "Inception score  (1.1699157, 0.0)\n",
      "[18/70][8/9] Loss_D: 0.4002 Loss_G: 0.2529 D(x): 0.5984 D(G(z)): 0.4389 / 0.5015\n",
      "Inception score  (1.0971783, 0.0)\n",
      "[19/70][8/9] Loss_D: 0.5165 Loss_G: 0.3567 D(x): 0.4777 D(G(z)): 0.4549 / 0.4069\n",
      "Inception score  (1.1502798, 0.0)\n",
      "[20/70][8/9] Loss_D: 0.4120 Loss_G: 0.2318 D(x): 0.6590 D(G(z)): 0.4998 / 0.5234\n",
      "Inception score  (1.1815267, 0.0)\n",
      "[21/70][8/9] Loss_D: 0.3634 Loss_G: 0.4564 D(x): 0.5459 D(G(z)): 0.3548 / 0.3291\n",
      "Inception score  (1.1504399, 0.0)\n",
      "[22/70][8/9] Loss_D: 0.2687 Loss_G: 0.6164 D(x): 0.5818 D(G(z)): 0.1887 / 0.2171\n",
      "Inception score  (1.0906173, 0.0)\n",
      "[23/70][8/9] Loss_D: 0.3080 Loss_G: 0.3565 D(x): 0.7145 D(G(z)): 0.4137 / 0.4089\n",
      "Inception score  (1.1737193, 0.0)\n",
      "[24/70][8/9] Loss_D: 0.3124 Loss_G: 0.4415 D(x): 0.6303 D(G(z)): 0.3381 / 0.3399\n",
      "Inception score  (1.1453865, 0.0)\n",
      "[25/70][8/9] Loss_D: 0.5018 Loss_G: 0.2530 D(x): 0.5837 D(G(z)): 0.5247 / 0.5094\n",
      "Inception score  (1.1837513, 0.0)\n",
      "[26/70][8/9] Loss_D: 0.3364 Loss_G: 0.3975 D(x): 0.6279 D(G(z)): 0.3954 / 0.3717\n",
      "Inception score  (1.1329559, 0.0)\n",
      "[27/70][8/9] Loss_D: 0.3094 Loss_G: 0.3263 D(x): 0.7074 D(G(z)): 0.4144 / 0.4323\n",
      "Inception score  (1.1964204, 0.0)\n",
      "[28/70][8/9] Loss_D: 0.1636 Loss_G: 0.5400 D(x): 0.7277 D(G(z)): 0.2187 / 0.2681\n",
      "Inception score  (1.2096894, 0.0)\n",
      "[29/70][8/9] Loss_D: 0.2541 Loss_G: 0.3603 D(x): 0.7509 D(G(z)): 0.3475 / 0.4063\n",
      "Inception score  (1.1856722, 0.0)\n",
      "[30/70][8/9] Loss_D: 0.3707 Loss_G: 0.4878 D(x): 0.5770 D(G(z)): 0.3446 / 0.3039\n",
      "Inception score  (1.3496279, 0.0)\n",
      "[31/70][8/9] Loss_D: 0.2659 Loss_G: 0.5200 D(x): 0.6198 D(G(z)): 0.2754 / 0.2823\n",
      "Inception score  (1.301831, 0.0)\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-9-205249352de2>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     38\u001b[0m             \u001b[0mfake\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnetG\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnoise\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     39\u001b[0m             \u001b[0moutput\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnetD\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfake\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 40\u001b[1;33m             \u001b[0merrG\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     41\u001b[0m             \u001b[0merrG\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     42\u001b[0m             \u001b[0mD_G_z2\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0moutput\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda2\\envs\\pytorch\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m    487\u001b[0m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    488\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 489\u001b[1;33m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    490\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    491\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda2\\envs\\pytorch\\lib\\site-packages\\torch\\nn\\modules\\loss.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, input, target)\u001b[0m\n\u001b[0;32m    433\u001b[0m     \u001b[1;33m@\u001b[0m\u001b[0mweak_script_method\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    434\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 435\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmse_loss\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreduction\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreduction\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    436\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    437\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda2\\envs\\pytorch\\lib\\site-packages\\torch\\nn\\functional.py\u001b[0m in \u001b[0;36mmse_loss\u001b[1;34m(input, target, size_average, reduce, reduction)\u001b[0m\n\u001b[0;32m   2154\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2155\u001b[0m         \u001b[0mexpanded_input\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mexpanded_target\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbroadcast_tensors\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2156\u001b[1;33m         \u001b[0mret\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_C\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_nn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmse_loss\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mexpanded_input\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mexpanded_target\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_Reduction\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_enum\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mreduction\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2157\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mret\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2158\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "ff_noise = torch.randn(1000, nz, 1, 1, device=device)\n",
    "\n",
    "print(\"Inception score \", inception_score(netG(ff_noise), inception_model))\n",
    "\n",
    "for epoch in range(opt.niter):\n",
    "    for i, data in enumerate(dataloader, 0):\n",
    "        ############################\n",
    "        # (1) Update D network: maximize log(D(x)) + log(1 - D(G(z)))\n",
    "        ###########################\n",
    "        # train with real\n",
    "        netD.zero_grad()\n",
    "        real_cpu = data[0].type(torch.FloatTensor).to(device)\n",
    "        batch_size = real_cpu.size(0)\n",
    "        label = torch.full((batch_size,), real_label, device=device)\n",
    "\n",
    "        output = netD(real_cpu)\n",
    "        errD_real = criterion(output, label)\n",
    "        errD_real.backward()\n",
    "        D_x = output.mean().item()\n",
    "\n",
    "        # train with fake\n",
    "        noise = torch.randn(batch_size, nz, 1, 1, device=device)\n",
    "        fake = netG(noise)\n",
    "        label.fill_(fake_label)\n",
    "        output = netD(fake.detach())\n",
    "        errD_fake = criterion(output, label)\n",
    "        errD_fake.backward()\n",
    "        D_G_z1 = output.mean().item()\n",
    "        errD = errD_real + errD_fake\n",
    "        optimizerD.step()\n",
    "\n",
    "        ############################\n",
    "        # (2) Update G network: maximize log(D(G(z)))\n",
    "        ###########################\n",
    "        for _ in range(2):\n",
    "            netG.zero_grad()\n",
    "            label.fill_(real_label)  # fake labels are real for generator cost\n",
    "            fake = netG(noise)\n",
    "            output = netD(fake)\n",
    "            errG = criterion(output, label)\n",
    "            errG.backward()\n",
    "            D_G_z2 = output.mean().item()\n",
    "            optimizerG.step()\n",
    "\n",
    "        print('[%d/%d][%d/%d] Loss_D: %.4f Loss_G: %.4f D(x): %.4f D(G(z)): %.4f / %.4f'\n",
    "              % (epoch, opt.niter, i, len(dataloader),\n",
    "                 errD.item(), errG.item(), D_x, D_G_z1, D_G_z2), end=\"\\r\")\n",
    "        if i % 100 == 0:\n",
    "            vutils.save_image(real_cpu,\n",
    "                    '%s/real_samples.png' % opt.outf,\n",
    "                    normalize=True)\n",
    "            fake = netG(fixed_noise)\n",
    "            vutils.save_image(fake.detach(),\n",
    "                    '%s/fake_samples_epoch_%03d.png' % (opt.outf, epoch),\n",
    "                    normalize=True)\n",
    "    print(\"\")\n",
    "    print(\"Inception score \", inception_score(netG(ff_noise), inception_model))\n",
    "\n",
    "    # do checkpointing\n",
    "    torch.save(netG.state_dict(), '%s/netG_epoch_%d.pth' % (opt.outf, epoch))\n",
    "    torch.save(netD.state_dict(), '%s/netD_epoch_%d.pth' % (opt.outf, epoch))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inception score  (2.6598134, 0.0)\n",
      "6717\n"
     ]
    }
   ],
   "source": [
    "print(\"Inception score \", inception_score(next(iter(dataloader))[0].type(torch.FloatTensor).to(device), inception_model))\n",
    "print(len(dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
